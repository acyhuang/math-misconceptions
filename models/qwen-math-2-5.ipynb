{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82695,"databundleVersionId":9738540,"sourceType":"competition"},{"sourceId":172510,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":146846,"modelId":169361},{"sourceId":172496,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":146837,"modelId":169361}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"local = False\nlog = True\nlog_detail = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:23.091447Z","iopub.execute_input":"2024-12-11T07:26:23.091781Z","iopub.status.idle":"2024-12-11T07:26:23.101936Z","shell.execute_reply.started":"2024-12-11T07:26:23.091740Z","shell.execute_reply":"2024-12-11T07:26:23.101127Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nimport os\nimport sys\nfrom tqdm import tqdm\nsys.path.append(os.path.abspath('..'))\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:23.104030Z","iopub.execute_input":"2024-12-11T07:26:23.104375Z","iopub.status.idle":"2024-12-11T07:26:23.991418Z","shell.execute_reply.started":"2024-12-11T07:26:23.104339Z","shell.execute_reply":"2024-12-11T07:26:23.990747Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:23.992379Z","iopub.execute_input":"2024-12-11T07:26:23.992737Z","iopub.status.idle":"2024-12-11T07:26:30.103922Z","shell.execute_reply.started":"2024-12-11T07:26:23.992707Z","shell.execute_reply":"2024-12-11T07:26:30.103257Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:30.105012Z","iopub.execute_input":"2024-12-11T07:26:30.105720Z","iopub.status.idle":"2024-12-11T07:26:30.110573Z","shell.execute_reply.started":"2024-12-11T07:26:30.105677Z","shell.execute_reply":"2024-12-11T07:26:30.109578Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"if local:\n    misconceptions = pd.read_csv('../kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv', index_col='MisconceptionId')\n    train = pd.read_csv('../kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv')\n    test = pd.read_csv('../kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv')\nelse:\n    misconceptions = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv', index_col='MisconceptionId')\n    train = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv')\n    test = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv')\nif log: print(\"(1) Imported data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:30.111803Z","iopub.execute_input":"2024-12-11T07:26:30.112218Z","iopub.status.idle":"2024-12-11T07:26:30.150024Z","shell.execute_reply.started":"2024-12-11T07:26:30.112178Z","shell.execute_reply":"2024-12-11T07:26:30.149263Z"}},"outputs":[{"name":"stdout","text":"(1) Imported data\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Define the identifier columns\nid_cols = [\n    'QuestionId', 'ConstructId', 'ConstructName', \n    'SubjectId', 'SubjectName', 'CorrectAnswer', 'QuestionText'\n]\n\n# Define the corresponding Answer options\nanswer_cols = ['AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText']\nmisconception_cols = ['MisconceptionAId', 'MisconceptionBId', 'MisconceptionCId', 'MisconceptionDId']\n\n# Melt Answer Text\ntext_melted = train.melt(\n    id_vars=id_cols,\n    value_vars=answer_cols,\n    var_name='Attribute',\n    value_name='AnswerText'\n)\n\n# Melt Misconception IDs\nmisconception_melted = train.melt(\n    id_vars=id_cols,\n    value_vars=misconception_cols,\n    var_name='Attribute',\n    value_name='MisconceptionId'\n)\n\n# Extract the option letter (A, B, C, D) and the attribute type\ntext_melted['AnswerOption'] = text_melted['Attribute'].str.extract(r'Answer([ABCD])Text')[0]\nmisconception_melted['AnswerOption'] = misconception_melted['Attribute'].str.extract(r'Misconception([ABCD])Id')[0]\n\n# Drop the original 'Attribute' columns as they are no longer needed\ntext_melted.drop('Attribute', axis=1, inplace=True)\nmisconception_melted.drop('Attribute', axis=1, inplace=True)\n\n# Merge the two melted DataFrames on id_vars and AnswerOption\ntrain_melted = pd.merge(\n    text_melted,\n    misconception_melted,\n    on=id_cols + ['AnswerOption'],\n    how='left'\n)\n\ntrain_melted = train_melted.merge(misconceptions, left_on='MisconceptionId', right_index=True, how='left')\nif log: print(\"(2) Created train_melted\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:30.150980Z","iopub.execute_input":"2024-12-11T07:26:30.151234Z","iopub.status.idle":"2024-12-11T07:26:30.198830Z","shell.execute_reply.started":"2024-12-11T07:26:30.151209Z","shell.execute_reply":"2024-12-11T07:26:30.198030Z"}},"outputs":[{"name":"stdout","text":"(2) Created train_melted\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"misconception_list = list(misconceptions['MisconceptionName'])\n\nsentence_model = SentenceTransformer('all-mpnet-base-v2')\nmisconception_embeddings = sentence_model.encode(misconception_list, convert_to_tensor=True).cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:30.200753Z","iopub.execute_input":"2024-12-11T07:26:30.201007Z","iopub.status.idle":"2024-12-11T07:26:36.783992Z","shell.execute_reply.started":"2024-12-11T07:26:30.200982Z","shell.execute_reply":"2024-12-11T07:26:36.783316Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/81 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03a4a3e133b64c468f54287017a1124d"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def match_misconception(response: str, subject: str, top_k: int = 3) -> list:\n    \"\"\"\n    Matches response to the top k misconceptions from the misconception list using word embeddings.\n    Returns a list of the top k matched misconceptions with their similarity scores.\n\n    Args:\n        response: The text to match against misconceptions\n        subject: The subject area for context\n        top_k: The number of top misconceptions to return\n\n    Returns:\n        list: A list of tuples containing the matched misconception and its similarity score\n    \"\"\"\n    # Combine response with subject for context\n    contextual_response = f\"{response} (Subject: {subject})\"\n    \n    # Compute embedding for the response and move to CPU, then convert to NumPy\n    response_embedding = model.encode([contextual_response], convert_to_tensor=True).cpu().numpy()\n    \n    # Calculate cosine similarities between the response and all misconceptions\n    similarities = cosine_similarity(response_embedding, misconception_embeddings)[0]\n    \n    # Get indices of top k similarities\n    top_indices = similarities.argsort()[-top_k:][::-1]\n    top_scores = similarities[top_indices]\n    \n    # Collect top k matches with their similarity scores\n    top_matches = []\n    for idx, score in zip(top_indices, top_scores):\n        top_matches.append((misconception_list[idx], score))\n    \n    return top_matches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:36.785121Z","iopub.execute_input":"2024-12-11T07:26:36.785497Z","iopub.status.idle":"2024-12-11T07:26:36.791951Z","shell.execute_reply.started":"2024-12-11T07:26:36.785457Z","shell.execute_reply":"2024-12-11T07:26:36.790990Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\nclass QwenMathModel:\n    def __init__(self, model_path: str = \"/kaggle/input/qwen2.5-math/transformers/1.5b-instruct/1\"):\n        \"\"\"\n        Initializes the Qwen Math 2.5 model and tokenizer.\n        Args:\n            model_name (str): The name of the model on Hugging Face Hub.\n        \"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n        self.model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n        self.model.eval()\n        if torch.cuda.is_available():\n            self.model.to('cuda')\n\n    def get_completion(self, prompt: str, max_tokens: int = 100) -> str:\n        \"\"\"\n        Generates a prediction for the given prompt.\n        Args:\n            prompt (str): The input prompt.\n            max_length (int): The maximum length of the generated sequence.\n        Returns:\n            str: The generated prediction.\n        \"\"\"\n        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n        if torch.cuda.is_available():\n            inputs = inputs.to('cuda')\n        with torch.no_grad():\n            outputs = self.model.generate(\n                inputs,\n                max_new_tokens=max_tokens,\n                num_return_sequences=1,\n                temperature=0.2,\n                top_p=0.95,\n                do_sample=True\n            )\n        prediction = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:36.792821Z","iopub.execute_input":"2024-12-11T07:26:36.793078Z","iopub.status.idle":"2024-12-11T07:26:36.805606Z","shell.execute_reply.started":"2024-12-11T07:26:36.793044Z","shell.execute_reply":"2024-12-11T07:26:36.804911Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def print_response(response, n=25):\n    for prediction in response[:n]:\n        print(f\"{prediction[1]:.4f} | {prediction[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:36.806685Z","iopub.execute_input":"2024-12-11T07:26:36.807473Z","iopub.status.idle":"2024-12-11T07:26:36.819433Z","shell.execute_reply.started":"2024-12-11T07:26:36.807445Z","shell.execute_reply":"2024-12-11T07:26:36.818556Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def calc_map25(predictions: list[tuple[str, float]], label: str, top_k: int = 25):\n    batch_size = len(predictions)\n    if batch_size == 0:\n        return 0.0\n    \n    # Get top k predictions by sorting\n    sorted_predictions = sorted(predictions, key=lambda x: x[1], reverse=True)[:top_k]\n    pred_labels = [label for label, score in sorted_predictions]\n    \n    # Calculate AP for the single sample\n    ap = 0.0\n    hits = 0\n    for j, pred_label in enumerate(pred_labels):\n        if pred_label == label:\n            hits += 1\n            ap += hits / (j + 1)\n            break  # Since there's only one correct label per observation\n    return ap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:36.820317Z","iopub.execute_input":"2024-12-11T07:26:36.820575Z","iopub.status.idle":"2024-12-11T07:26:36.829357Z","shell.execute_reply.started":"2024-12-11T07:26:36.820551Z","shell.execute_reply":"2024-12-11T07:26:36.828627Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model = QwenMathModel()\n\n\nfor index, row in train_melted.tail(1).iterrows():\n    subject = row['SubjectName']\n    question_text = row['QuestionText']\n    answer_text = row['AnswerText']\n    prompt = f\"\"\"Given the following question and incorrect answer option, identify the underlying misconception that would cause someone to arrive at the answer and output it as your response in one sentence.\nSubject: {subject}\nQuestion: {question_text}\nAnswer: {answer_text}\"\"\"\n    \n    raw_response = model.get_completion(prompt, max_tokens=100)\n    response = match_misconception(raw_response, subject, top_k=25)\n\n    print(\"-\"*100)\n    print(f\"QuestionID={row['QuestionId']}, AnswerOption={row['AnswerOption']}\")\n    print(\"PROMPT\")\n    print(prompt)\n    print()\n    print(\"RESPONSE\")\n    print(f\"Raw: {raw_response}\")\n    print(\"Top 5 predictions:\")\n    print_response(response, n=5)\n    print()\n    print(f\"ACTUAL, MAP@25: {calc_map25(response, row['MisconceptionName'])}\")\n    print(row['MisconceptionName'])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:36.830366Z","iopub.execute_input":"2024-12-11T07:26:36.830681Z","iopub.status.idle":"2024-12-11T07:26:46.208585Z","shell.execute_reply.started":"2024-12-11T07:26:36.830646Z","shell.execute_reply":"2024-12-11T07:26:46.207397Z"}},"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGiven the following question and incorrect answer option, identify the underlying misconception that would cause someone to arrive at the answer and output it as your response in one sentence.\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mSubject: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion_text\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     13\u001b[0m     raw_response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_completion(prompt, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmatch_misconception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestionID=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestionId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, AnswerOption=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswerOption\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36mmatch_misconception\u001b[0;34m(response, subject, top_k)\u001b[0m\n\u001b[1;32m     15\u001b[0m contextual_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Subject: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Compute embedding for the response and move to CPU, then convert to NumPy\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m response_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m([contextual_response], convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarities between the response and all misconceptions\u001b[39;00m\n\u001b[1;32m     21\u001b[0m similarities \u001b[38;5;241m=\u001b[39m cosine_similarity(response_embedding, misconception_embeddings)[\u001b[38;5;241m0\u001b[39m]\n","\u001b[0;31mAttributeError\u001b[0m: 'QwenMathModel' object has no attribute 'encode'"],"ename":"AttributeError","evalue":"'QwenMathModel' object has no attribute 'encode'","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"#########################\n# Create test_melted \n#########################\n\n# Include 'QuestionText' in id_vars to preserve it in the melted DataFrame\ntest_melted = test.melt(\n    id_vars=['QuestionId', 'QuestionText', 'CorrectAnswer'],\n    value_vars=['AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText'],\n    var_name='AnswerOption',\n    value_name='AnswerText'\n)\n\n# Clean the 'AnswerOption' column to obtain A, B, C, D\ntest_melted['AnswerOption'] = test_melted['AnswerOption'].str.replace('Answer', '').str.replace('Text', '')\n\ntest_melted['QA_Id'] = test_melted['QuestionId'].astype(str) + '_' + test_melted['AnswerOption']\n\n# Drop correct answers\ntest_melted = test_melted[test_melted['CorrectAnswer'] != test_melted['AnswerOption']]\n\ntest_melted.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T07:26:46.209509Z","iopub.status.idle":"2024-12-11T07:26:46.209851Z","shell.execute_reply.started":"2024-12-11T07:26:46.209694Z","shell.execute_reply":"2024-12-11T07:26:46.209710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}