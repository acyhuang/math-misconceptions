{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:28:20.486477Z","iopub.status.busy":"2024-12-11T04:28:20.486130Z","iopub.status.idle":"2024-12-11T04:28:20.510714Z","shell.execute_reply":"2024-12-11T04:28:20.510017Z","shell.execute_reply.started":"2024-12-11T04:28:20.486446Z"},"trusted":true},"outputs":[],"source":["local = True\n","\n","log = True\n","\n","log_detail = False"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"markdown","metadata":{},"source":["## Libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:15.734356Z","iopub.status.busy":"2024-12-11T04:29:15.733570Z","iopub.status.idle":"2024-12-11T04:29:18.364262Z","shell.execute_reply":"2024-12-11T04:29:18.363434Z","shell.execute_reply.started":"2024-12-11T04:29:15.734317Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","\n","import gc\n","import os\n","import sys\n","from tqdm import tqdm\n","sys.path.append(os.path.abspath('..'))\n","import importlib\n","import pickle"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:18.366871Z","iopub.status.busy":"2024-12-11T04:29:18.366006Z","iopub.status.idle":"2024-12-11T04:29:18.371186Z","shell.execute_reply":"2024-12-11T04:29:18.370251Z","shell.execute_reply.started":"2024-12-11T04:29:18.366826Z"},"trusted":true},"outputs":[],"source":["os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:18.372769Z","iopub.status.busy":"2024-12-11T04:29:18.372411Z","iopub.status.idle":"2024-12-11T04:29:36.608334Z","shell.execute_reply":"2024-12-11T04:29:36.607463Z","shell.execute_reply.started":"2024-12-11T04:29:18.372731Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/allison/Downloads/idsn544/math-misconceptions/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import average_precision_score\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import AdamW"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:36.610637Z","iopub.status.busy":"2024-12-11T04:29:36.610017Z","iopub.status.idle":"2024-12-11T04:29:36.615168Z","shell.execute_reply":"2024-12-11T04:29:36.614243Z","shell.execute_reply.started":"2024-12-11T04:29:36.610606Z"},"trusted":true},"outputs":[],"source":["pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', None)"]},{"cell_type":"markdown","metadata":{},"source":["## Data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:36.616966Z","iopub.status.busy":"2024-12-11T04:29:36.616530Z","iopub.status.idle":"2024-12-11T04:29:36.702295Z","shell.execute_reply":"2024-12-11T04:29:36.701347Z","shell.execute_reply.started":"2024-12-11T04:29:36.616913Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(1) Imported data\n"]}],"source":["if local:\n","\n","    misconceptions = pd.read_csv('../kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv', index_col='MisconceptionId')\n","    train = pd.read_csv('../kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv')\n","    test = pd.read_csv('../kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv')\n","\n","else:\n","\n","    misconceptions = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv', index_col='MisconceptionId')\n","\n","    train = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv')\n","\n","    test = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv')\n","\n","if log: print(\"(1) Imported data\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:36.703716Z","iopub.status.busy":"2024-12-11T04:29:36.703395Z","iopub.status.idle":"2024-12-11T04:29:36.771344Z","shell.execute_reply":"2024-12-11T04:29:36.770318Z","shell.execute_reply.started":"2024-12-11T04:29:36.703687Z"},"trusted":true},"outputs":[],"source":["#########################\n","\n","# Create train_melted \n","\n","#########################\n","\n","\n","\n","# Define the identifier columns\n","\n","id_cols = [\n","\n","    'QuestionId', 'ConstructId', 'ConstructName', \n","\n","    'SubjectId', 'SubjectName', 'CorrectAnswer', 'QuestionText'\n","\n","]\n","\n","\n","\n","# Define the corresponding Answer options\n","\n","answer_cols = ['AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText']\n","\n","misconception_cols = ['MisconceptionAId', 'MisconceptionBId', 'MisconceptionCId', 'MisconceptionDId']\n","\n","\n","\n","# Melt Answer Text\n","\n","text_melted = train.melt(\n","\n","    id_vars=id_cols,\n","\n","    value_vars=answer_cols,\n","\n","    var_name='Attribute',\n","\n","    value_name='AnswerText'\n","\n",")\n","\n","\n","\n","# Melt Misconception IDs\n","\n","misconception_melted = train.melt(\n","\n","    id_vars=id_cols,\n","\n","    value_vars=misconception_cols,\n","\n","    var_name='Attribute',\n","\n","    value_name='MisconceptionId'\n","\n",")\n","\n","\n","\n","# Extract the option letter (A, B, C, D) and the attribute type\n","\n","text_melted['AnswerOption'] = text_melted['Attribute'].str.extract(r'Answer([ABCD])Text')[0]\n","\n","misconception_melted['AnswerOption'] = misconception_melted['Attribute'].str.extract(r'Misconception([ABCD])Id')[0]\n","\n","\n","\n","# Drop the original 'Attribute' columns as they are no longer needed\n","\n","text_melted.drop('Attribute', axis=1, inplace=True)\n","\n","misconception_melted.drop('Attribute', axis=1, inplace=True)\n","\n","\n","\n","# Merge the two melted DataFrames on id_vars and AnswerOption\n","\n","train_melted = pd.merge(\n","\n","    text_melted,\n","\n","    misconception_melted,\n","\n","    on=id_cols + ['AnswerOption'],\n","\n","    how='left'\n","\n",")\n","\n","\n","\n","train_melted = train_melted.merge(misconceptions, left_on='MisconceptionId', right_index=True, how='left')\n","\n","train_melted = train_melted[train_melted['CorrectAnswer'] != train_melted['AnswerOption']]"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:36.772946Z","iopub.status.busy":"2024-12-11T04:29:36.772632Z","iopub.status.idle":"2024-12-11T04:29:36.780926Z","shell.execute_reply":"2024-12-11T04:29:36.780006Z","shell.execute_reply.started":"2024-12-11T04:29:36.772915Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["SubjectName nunique: 163\n","ConstructName nunique: 757\n","MisconceptionName nunique: 1604\n"]}],"source":["print(f\"SubjectName nunique: {train_melted['SubjectName'].nunique()}\")\n","\n","print(f\"ConstructName nunique: {train_melted['ConstructName'].nunique()}\")\n","\n","print(f\"MisconceptionName nunique: {train_melted['MisconceptionName'].nunique()}\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:36.782307Z","iopub.status.busy":"2024-12-11T04:29:36.782001Z","iopub.status.idle":"2024-12-11T04:29:37.368024Z","shell.execute_reply":"2024-12-11T04:29:37.367272Z","shell.execute_reply.started":"2024-12-11T04:29:36.782279Z"},"trusted":true},"outputs":[],"source":["hierarchy_scm = {}\n","\n","for subject in train_melted['SubjectName'].unique():\n","\n","    subject_data = train_melted[train_melted['SubjectName'] == subject]\n","\n","    constructs = subject_data['ConstructName'].dropna().unique()\n","\n","    constructs_list = []\n","\n","    for construct in constructs:\n","\n","        misconceptions_list = subject_data[subject_data['ConstructName'] == construct]['MisconceptionName'].dropna().unique().tolist()\n","\n","        constructs_list.extend(misconceptions_list)\n","\n","    hierarchy_scm[subject] = constructs_list"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["163\n","Average number of misconceptions per subject: 16.81\n"]}],"source":["print(len(hierarchy_scm))\n","avg_len = sum(len(misconceptions) for misconceptions in hierarchy_scm.values()) / len(hierarchy_scm)\n","print(f\"Average number of misconceptions per subject: {avg_len:.2f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Classes"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:37.369541Z","iopub.status.busy":"2024-12-11T04:29:37.369211Z","iopub.status.idle":"2024-12-11T04:29:37.375641Z","shell.execute_reply":"2024-12-11T04:29:37.374502Z","shell.execute_reply.started":"2024-12-11T04:29:37.369510Z"},"trusted":true},"outputs":[],"source":["class Hierarchy:\n","\n","    def __init__(self):\n","\n","        self.parent_to_children = {}\n","\n","        for subject, misconceptions in hierarchy_scm.items():\n","\n","            self.parent_to_children[subject] = misconceptions\n","\n","        self.child_to_parent = {child: parent for parent, children in self.parent_to_children.items() for child in children}\n","\n","    \n","\n","    def get_parent(self, child):\n","\n","        return self.child_to_parent.get(child, None)\n","\n","    \n","\n","    def get_children(self, parent):\n","\n","        return self.parent_to_children.get(parent, [])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:37.378577Z","iopub.status.busy":"2024-12-11T04:29:37.378304Z","iopub.status.idle":"2024-12-11T04:29:37.387510Z","shell.execute_reply":"2024-12-11T04:29:37.386617Z","shell.execute_reply.started":"2024-12-11T04:29:37.378551Z"},"trusted":true},"outputs":[],"source":["class DataPreprocessor:\n","\n","    def __init__(self, train_df, test_df, hierarchy):\n","\n","        self.train_df = train_df\n","\n","        self.test_df = test_df\n","\n","        self.hierarchy = hierarchy\n","\n","        self.label_encoders = {}\n","\n","        self.child_label_encoders = {}\n","\n","    \n","\n","    def map_hierarchy(self):\n","\n","        self.train_df['ParentCategory'] = self.train_df['MisconceptionName'].apply(self.hierarchy.get_parent)\n","\n","        self.test_df['ParentCategory'] = None  # To be predicted later\n","\n","    \n","\n","    def encode_labels(self):\n","\n","        # Encode parent categories\n","\n","        le_parent = LabelEncoder()\n","\n","        self.train_df['ParentCategoryEncoded'] = le_parent.fit_transform(self.train_df['ParentCategory'])\n","\n","        self.label_encoders['parent'] = le_parent\n","\n","        \n","\n","        # Encode specific misconceptions per parent\n","\n","        self.child_label_encoders = {}\n","\n","        self.train_df['MisconceptionEncoded'] = -1  # Initialize with a placeholder\n","\n","        \n","\n","        for parent in self.hierarchy.parent_to_children.keys():\n","\n","            misconception_subset = self.train_df[self.train_df['ParentCategory'] == parent]['MisconceptionName']\n","\n","            le_child = LabelEncoder()\n","\n","            encoded = le_child.fit_transform(misconception_subset)\n","\n","            self.train_df.loc[self.train_df['ParentCategory'] == parent, 'MisconceptionEncoded'] = encoded\n","\n","            self.child_label_encoders[parent] = le_child\n","\n","        \n","\n","    def preprocess(self):\n","\n","        self.map_hierarchy()\n","\n","        self.encode_labels()\n","\n","        # Additional preprocessing steps like text cleaning can be added here\n","\n","        return self.train_df, self.test_df"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:37.388984Z","iopub.status.busy":"2024-12-11T04:29:37.388671Z","iopub.status.idle":"2024-12-11T04:29:37.407337Z","shell.execute_reply":"2024-12-11T04:29:37.406652Z","shell.execute_reply.started":"2024-12-11T04:29:37.388920Z"},"trusted":true},"outputs":[],"source":["class HierarchicalClassifier:\n","    def __init__(self, parent_num_labels, pretrained_model='/kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594'):\n","        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n","        self.parent_model = None\n","        self.child_models = {}  # Dictionary to store child models\n","        self.pretrained_model = pretrained_model\n","        self.parent_num_labels = parent_num_labels\n","    \n","    def load_parent_model(self, device):\n","        if self.parent_model is not None:\n","            self.parent_model.cpu()\n","            del self.parent_model\n","            torch.cuda.empty_cache()\n","        \n","        self.parent_model = BertForSequenceClassification.from_pretrained(\n","            self.pretrained_model, \n","            num_labels=self.parent_num_labels\n","        ).to(device)\n","    \n","    def load_child_model(self, parent_category, num_labels, device):\n","        # Clear only this specific child model if it exists\n","        if parent_category in self.child_models:\n","            self.child_models[parent_category].cpu()\n","            del self.child_models[parent_category]\n","            torch.cuda.empty_cache()\n","        \n","        child_model = BertForSequenceClassification.from_pretrained(\n","            self.pretrained_model, \n","            num_labels=num_labels\n","        ).to(device)\n","        \n","        self.child_models[parent_category] = child_model\n","        return child_model\n","    \n","    def save_model(self, path):\n","        os.makedirs(path, exist_ok=True)\n","        \n","        if self.parent_model is not None:\n","            self.parent_model.save_pretrained(os.path.join(path, 'parent_model'))\n","        \n","        for parent_key, child_model in self.child_models.items():\n","            safe_key = str(parent_key).replace(\" \", \"_\").replace(\"/\", \"_\")\n","            child_model.save_pretrained(os.path.join(path, f'child_model_{safe_key}'))\n","    \n","    def load_model(self, path):\n","        parent_path = os.path.join(path, 'parent_model')\n","        if os.path.exists(parent_path):\n","            self.parent_model = BertForSequenceClassification.from_pretrained(parent_path)\n","        \n","        for model_dir in os.listdir(path):\n","            if model_dir.startswith('child_model_'):\n","                parent_key = model_dir.replace('child_model_', '').replace(\"_\", \" \")\n","                model_path = os.path.join(path, model_dir)\n","                self.child_models[parent_key] = BertForSequenceClassification.from_pretrained(model_path)\n","\n","    def clear_models(self):\n","        \"\"\"Clear all models from memory\"\"\"\n","        if self.parent_model is not None:\n","            self.parent_model.cpu()\n","            del self.parent_model\n","            self.parent_model = None\n","        \n","        for key in list(self.child_models.keys()):\n","            self.child_models[key].cpu()\n","            del self.child_models[key]\n","        self.child_models.clear()\n","        \n","        torch.cuda.empty_cache()\n","        gc.collect()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:37.408733Z","iopub.status.busy":"2024-12-11T04:29:37.408472Z","iopub.status.idle":"2024-12-11T04:29:37.427614Z","shell.execute_reply":"2024-12-11T04:29:37.426697Z","shell.execute_reply.started":"2024-12-11T04:29:37.408707Z"},"trusted":true},"outputs":[],"source":["class DistractorDataset(Dataset):\n","\n","    def __init__(self, texts, labels, tokenizer, max_length=64):\n","\n","        self.texts = texts\n","\n","        self.labels = labels\n","\n","        self.tokenizer = tokenizer\n","\n","        self.max_length = max_length\n","\n","    \n","\n","    def __len__(self):\n","\n","        return len(self.texts)\n","\n","    \n","\n","    def __getitem__(self, idx):\n","\n","        encoding = self.tokenizer.encode_plus(\n","\n","            self.texts[idx],\n","\n","            add_special_tokens=True,\n","\n","            max_length=self.max_length,\n","\n","            padding='max_length',\n","\n","            truncation=True,\n","\n","            return_attention_mask=True,\n","\n","            return_tensors='pt'\n","\n","        )\n","\n","        return {\n","\n","            'input_ids': encoding['input_ids'].flatten(),\n","\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","\n","            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        }\n","\n","\n","def train_stage1(model, train_df, tokenizer, device, epochs=3):\n","    # Load parent model\n","    model.load_parent_model(device)\n","    \n","    dataset = DistractorDataset(\n","        texts=train_df['AnswerText'].tolist(),\n","        labels=train_df['ParentCategoryEncoded'].tolist(),\n","        tokenizer=tokenizer,\n","        max_length=64  # Reduced max length\n","    )\n","    \n","    # Smaller batch size with gradient accumulation\n","    batch_size = 4\n","    gradient_accumulation_steps = 8\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    \n","    optimizer = torch.optim.AdamW(model.parent_model.parameters(), lr=2e-5)\n","    \n","    for epoch in range(epochs):\n","        total_loss = 0.0\n","        optimizer.zero_grad()\n","        \n","        for i, batch in enumerate(tqdm(dataloader)):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            \n","            outputs = model.parent_model(\n","                input_ids=input_ids, \n","                attention_mask=attention_mask, \n","                labels=labels\n","            )\n","            \n","            loss = outputs.loss / gradient_accumulation_steps\n","            loss.backward()\n","            \n","            if (i + 1) % gradient_accumulation_steps == 0:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                torch.cuda.empty_cache()\n","            \n","            # Free memory\n","            del outputs, input_ids, attention_mask, labels\n","            \n","            total_loss += loss.item() * gradient_accumulation_steps\n","        \n","        avg_loss = total_loss / len(dataloader)\n","        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n","\n","def train_stage2(model, train_df, tokenizer, hierarchy, device, epochs=3):\n","    # Explicitly clear parent model and GPU cache\n","    model.clear_models()\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","    for parent in tqdm(hierarchy.parent_to_children.keys()):\n","        parent_key = str(parent).replace(\" \", \"_\").replace(\"/\", \"_\")\n","        child_subset = train_df[train_df['ParentCategory'] == parent]\n","        \n","        if child_subset.empty:\n","            continue\n","            \n","        num_labels = len(child_subset['MisconceptionEncoded'].unique())\n","        \n","        try:\n","            # Load child model for this parent\n","            child_model = model.load_child_model(parent, num_labels, device)\n","            \n","            dataset = DistractorDataset(\n","                texts=child_subset['AnswerText'].tolist(),\n","                labels=child_subset['MisconceptionEncoded'].tolist(),\n","                tokenizer=tokenizer,\n","                max_length=64\n","            )\n","            \n","            # Further reduced batch size and increased gradient accumulation\n","            batch_size = 1\n","            gradient_accumulation_steps = 32\n","            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","            \n","            optimizer = torch.optim.AdamW(child_model.parameters(), lr=2e-5)\n","            \n","            for epoch in range(epochs):\n","                total_loss = 0\n","                optimizer.zero_grad()\n","                \n","                for i, batch in enumerate(dataloader):\n","                    # Move batch to CPU first to clear GPU memory\n","                    input_ids = batch['input_ids'].to(device)\n","                    attention_mask = batch['attention_mask'].to(device)\n","                    labels = batch['labels'].long().to(device)\n","                    \n","                    # Clear cache before forward pass\n","                    torch.cuda.empty_cache()\n","                    \n","                    outputs = child_model(\n","                        input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                        labels=labels\n","                    )\n","                    \n","                    loss = outputs.loss / gradient_accumulation_steps\n","                    loss.backward()\n","                    \n","                    if (i + 1) % gradient_accumulation_steps == 0:\n","                        optimizer.step()\n","                        optimizer.zero_grad()\n","                    \n","                    # Explicitly clear memory after each batch\n","                    del outputs, input_ids, attention_mask, labels\n","                    torch.cuda.empty_cache()\n","                    \n","                    total_loss += loss.item() * gradient_accumulation_steps\n","                \n","                avg_loss = total_loss / len(dataloader)\n","                if log:\n","                    print(f\"Parent: {parent}, Epoch: {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n","                \n","                # Clear cache after each epoch\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","            \n","            # Clear model from GPU after training for this parent\n","            child_model.cpu()\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","                \n","        except Exception as e:\n","            print(f\"Error training model for {parent}: {e}\")\n","            # Ensure cleanup even if error occurs\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            continue\n","        \n","        # Additional cleanup between parents\n","        torch.cuda.empty_cache()\n","        gc.collect()"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:37.428852Z","iopub.status.busy":"2024-12-11T04:29:37.428604Z","iopub.status.idle":"2024-12-11T04:29:37.443135Z","shell.execute_reply":"2024-12-11T04:29:37.442337Z","shell.execute_reply.started":"2024-12-11T04:29:37.428821Z"},"trusted":true},"outputs":[],"source":["def predict(model, texts, hierarchy, tokenizer, label_encoders, device):\n","\n","    dataset = DistractorDataset(\n","\n","        texts=texts,\n","\n","        labels=[0]*len(texts),  # Dummy labels\n","\n","        tokenizer=tokenizer\n","\n","    )\n","\n","    dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n","\n","    model.parent_model.to(device)\n","\n","    model.parent_model.eval()\n","\n","    \n","\n","    parent_preds = []\n","\n","    with torch.no_grad():\n","\n","        for batch in dataloader:\n","\n","            input_ids = batch['input_ids'].to(device)\n","\n","            attention_mask = batch['attention_mask'].to(device)\n","\n","            outputs = model.parent_model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","            logits = outputs.logits\n","\n","            preds = torch.argmax(logits, dim=1).cpu().numpy()\n","\n","            parent_preds.extend(preds)\n","\n","    \n","\n","    parent_labels = label_encoders['parent'].inverse_transform(parent_preds)\n","\n","    specific_preds = []\n","\n","    \n","\n","    for i, parent in enumerate(parent_labels):\n","\n","        child_model = model.child_models.get(parent, None)\n","\n","        if child_model is None:\n","\n","            specific_preds.append(None)\n","\n","            continue\n","\n","        child_model.to(device)\n","\n","        child_model.eval()\n","\n","        with torch.no_grad():\n","\n","            encoding = tokenizer.encode_plus(\n","\n","                texts[i],\n","\n","                add_special_tokens=True,\n","\n","                max_length=128,\n","\n","                padding='max_length',\n","\n","                truncation=True,\n","\n","                return_attention_mask=True,\n","\n","                return_tensors='pt'\n","\n","            )\n","\n","            input_ids = encoding['input_ids'].to(device)\n","\n","            attention_mask = encoding['attention_mask'].to(device)\n","\n","            outputs = child_model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","            logits = outputs.logits\n","\n","            pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n","\n","            # Use the specific encoder for the parent\n","\n","            specific_misconception = label_encoders['child_label_encoders'][parent].inverse_transform([pred])[0]\n","\n","            specific_preds.append(specific_misconception)\n","\n","    \n","\n","    return specific_preds"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:37.444323Z","iopub.status.busy":"2024-12-11T04:29:37.444016Z","iopub.status.idle":"2024-12-11T04:29:37.458218Z","shell.execute_reply":"2024-12-11T04:29:37.457254Z","shell.execute_reply.started":"2024-12-11T04:29:37.444295Z"},"trusted":true},"outputs":[],"source":["hierarchy = Hierarchy()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:37.459657Z","iopub.status.busy":"2024-12-11T04:29:37.459337Z","iopub.status.idle":"2024-12-11T04:29:37.468779Z","shell.execute_reply":"2024-12-11T04:29:37.467840Z","shell.execute_reply.started":"2024-12-11T04:29:37.459627Z"},"trusted":true},"outputs":[],"source":["def calculate_map25(true_labels, pred_probs, k=25):\n","    \"\"\"Calculate MAP@25 for predictions\"\"\"\n","    # Convert predictions to probabilities if they aren't already\n","    if isinstance(pred_probs, torch.Tensor):\n","        pred_probs = pred_probs.cpu().numpy()\n","    \n","    # Handle case where we have fewer than k classes\n","    k = min(k, pred_probs.shape[1])\n","    \n","    # Calculate AP for each sample and average\n","    aps = []\n","    for i in range(len(true_labels)):\n","        y_true = np.zeros(pred_probs.shape[1])\n","        y_true[true_labels[i]] = 1\n","        ap = average_precision_score(y_true, pred_probs[i])\n","        aps.append(ap)\n","    \n","    return np.mean(aps)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:37.470016Z","iopub.status.busy":"2024-12-11T04:29:37.469752Z","iopub.status.idle":"2024-12-11T04:29:37.837627Z","shell.execute_reply":"2024-12-11T04:29:37.836899Z","shell.execute_reply.started":"2024-12-11T04:29:37.469990Z"},"trusted":true},"outputs":[],"source":["# Define paths to your data files\n","\n","train_path = '../kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv'\n","\n","test_path = '../kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv'\n","\n","\n","\n","# Initialize DataPreprocessor\n","\n","preprocessor = DataPreprocessor(train_df=train_melted, test_df=test, hierarchy=hierarchy)\n","\n","train_df, test_df = preprocessor.preprocess()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:39.547263Z","iopub.status.busy":"2024-12-11T04:29:39.546774Z","iopub.status.idle":"2024-12-11T04:29:39.648051Z","shell.execute_reply":"2024-12-11T04:29:39.647322Z","shell.execute_reply.started":"2024-12-11T04:29:39.547215Z"},"trusted":true},"outputs":[],"source":["# Determine the number of parent labels\n","\n","parent_num_labels = train_df['ParentCategoryEncoded'].nunique()\n","\n","\n","\n","# Initialize HierarchicalClassifier\n","\n","model = HierarchicalClassifier(parent_num_labels=parent_num_labels) "]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:39.846362Z","iopub.status.busy":"2024-12-11T04:29:39.845986Z","iopub.status.idle":"2024-12-11T04:29:39.921310Z","shell.execute_reply":"2024-12-11T04:29:39.919920Z","shell.execute_reply.started":"2024-12-11T04:29:39.846326Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","print(f'Using device: {device}')"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:29:52.145247Z","iopub.status.busy":"2024-12-11T04:29:52.144849Z","iopub.status.idle":"2024-12-11T04:30:53.583138Z","shell.execute_reply":"2024-12-11T04:30:53.582292Z","shell.execute_reply.started":"2024-12-11T04:29:52.145210Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 1402/1402 [01:00<00:00, 23.27it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/1 - Loss: 4.4212\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Train Stage 1: Parent Category Classification\n","\n","train_stage1(model, train_df, model.tokenizer, device, epochs=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-11T04:32:51.009799Z","iopub.status.busy":"2024-12-11T04:32:51.008955Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/163 [00:00<?, ?it/s]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Simplifying Algebraic Fractions, Epoch: 1/3, Loss: 2.2835\n","Parent: Simplifying Algebraic Fractions, Epoch: 2/3, Loss: 2.2835\n","Parent: Simplifying Algebraic Fractions, Epoch: 3/3, Loss: 2.2835\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 1/163 [00:03<10:01,  3.71s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Range and Interquartile Range from a List of Data, Epoch: 1/3, Loss: 2.3678\n","Parent: Range and Interquartile Range from a List of Data, Epoch: 2/3, Loss: 2.3678\n","Parent: Range and Interquartile Range from a List of Data, Epoch: 3/3, Loss: 2.3678\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 2/163 [00:08<11:45,  4.38s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Properties of Quadrilaterals, Epoch: 1/3, Loss: 2.5455\n","Parent: Properties of Quadrilaterals, Epoch: 2/3, Loss: 2.5455\n","Parent: Properties of Quadrilaterals, Epoch: 3/3, Loss: 2.5455\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 3/163 [00:13<12:19,  4.62s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Area of Simple Shapes, Epoch: 1/3, Loss: 3.4294\n","Parent: Area of Simple Shapes, Epoch: 2/3, Loss: 3.3754\n","Parent: Area of Simple Shapes, Epoch: 3/3, Loss: 3.2906\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 4/163 [00:21<16:10,  6.11s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Converting between Fractions and Percentages, Epoch: 1/3, Loss: 2.1624\n","Parent: Converting between Fractions and Percentages, Epoch: 2/3, Loss: 2.1624\n","Parent: Converting between Fractions and Percentages, Epoch: 3/3, Loss: 2.1624\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 5/163 [00:25<14:04,  5.35s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Transformations of functions in the form f(x), Epoch: 1/3, Loss: 2.1626\n","Parent: Transformations of functions in the form f(x), Epoch: 2/3, Loss: 2.1626\n","Parent: Transformations of functions in the form f(x), Epoch: 3/3, Loss: 2.1626\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▎         | 6/163 [00:29<12:40,  4.85s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  4%|▍         | 7/163 [00:30<09:02,  3.48s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Error training model for Expanding Triple Brackets and more: Found dtype Long but expected Float\n","Parent: Nets, Epoch: 1/3, Loss: 0.9994\n","Parent: Nets, Epoch: 2/3, Loss: 0.9994\n","Parent: Nets, Epoch: 3/3, Loss: 0.9994\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▍         | 8/163 [00:34<09:36,  3.72s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Time, Epoch: 1/3, Loss: 3.1830\n","Parent: Time, Epoch: 2/3, Loss: 3.1252\n","Parent: Time, Epoch: 3/3, Loss: 3.0649\n"]},{"name":"stderr","output_type":"stream","text":["  6%|▌         | 9/163 [00:42<13:11,  5.14s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Trial and Improvement and Iterative Methods, Epoch: 1/3, Loss: 1.3655\n","Parent: Trial and Improvement and Iterative Methods, Epoch: 2/3, Loss: 1.3655\n","Parent: Trial and Improvement and Iterative Methods, Epoch: 3/3, Loss: 1.3655\n"]},{"name":"stderr","output_type":"stream","text":["  6%|▌         | 10/163 [00:46<11:43,  4.60s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Sharing in a Ratio, Epoch: 1/3, Loss: 2.6757\n","Parent: Sharing in a Ratio, Epoch: 2/3, Loss: 2.6757\n","Parent: Sharing in a Ratio, Epoch: 3/3, Loss: 2.6757\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 11/163 [00:51<12:09,  4.80s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Ordering Negative Numbers, Epoch: 1/3, Loss: 1.8653\n","Parent: Ordering Negative Numbers, Epoch: 2/3, Loss: 1.8653\n","Parent: Ordering Negative Numbers, Epoch: 3/3, Loss: 1.8653\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 12/163 [00:55<11:04,  4.40s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Adding and Subtracting Negative Numbers, Epoch: 1/3, Loss: 2.8260\n","Parent: Adding and Subtracting Negative Numbers, Epoch: 2/3, Loss: 2.8260\n","Parent: Adding and Subtracting Negative Numbers, Epoch: 3/3, Loss: 2.8260\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 13/163 [01:00<11:39,  4.66s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Adding and Subtracting Algebraic Fractions, Epoch: 1/3, Loss: 1.8472\n","Parent: Adding and Subtracting Algebraic Fractions, Epoch: 2/3, Loss: 1.8472\n","Parent: Adding and Subtracting Algebraic Fractions, Epoch: 3/3, Loss: 1.8472\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▊         | 14/163 [01:03<10:32,  4.24s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Volume of Prisms, Epoch: 1/3, Loss: 2.7163\n","Parent: Volume of Prisms, Epoch: 2/3, Loss: 2.6434\n","Parent: Volume of Prisms, Epoch: 3/3, Loss: 2.5522\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 15/163 [01:09<11:43,  4.75s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Factorising into a Double Bracket, Epoch: 1/3, Loss: 2.5363\n","Parent: Factorising into a Double Bracket, Epoch: 2/3, Loss: 2.3599\n","Parent: Factorising into a Double Bracket, Epoch: 3/3, Loss: 2.2272\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|▉         | 16/163 [01:16<13:06,  5.35s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Mental Multiplication and Division, Epoch: 1/3, Loss: 2.7974\n","Parent: Mental Multiplication and Division, Epoch: 2/3, Loss: 2.7974\n","Parent: Mental Multiplication and Division, Epoch: 3/3, Loss: 2.7974\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 17/163 [01:21<12:49,  5.27s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Substitution into Formula, Epoch: 1/3, Loss: 2.3666\n","Parent: Substitution into Formula, Epoch: 2/3, Loss: 2.3666\n","Parent: Substitution into Formula, Epoch: 3/3, Loss: 2.3666\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 18/163 [01:26<12:38,  5.23s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Counting, Epoch: 1/3, Loss: 0.7669\n","Parent: Counting, Epoch: 2/3, Loss: 0.7669\n","Parent: Counting, Epoch: 3/3, Loss: 0.7669\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 19/163 [01:29<11:19,  4.72s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Solving Linear Inequalities, Epoch: 1/3, Loss: 2.1333\n","Parent: Solving Linear Inequalities, Epoch: 2/3, Loss: 2.1333\n","Parent: Solving Linear Inequalities, Epoch: 3/3, Loss: 2.1333\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 20/163 [01:34<10:57,  4.60s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Linear Equations, Epoch: 1/3, Loss: 3.0851\n","Parent: Linear Equations, Epoch: 2/3, Loss: 3.0137\n","Parent: Linear Equations, Epoch: 3/3, Loss: 2.9321\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 21/163 [01:41<12:32,  5.30s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Real Life Graphs, Epoch: 1/3, Loss: 3.1564\n","Parent: Real Life Graphs, Epoch: 2/3, Loss: 3.0086\n","Parent: Real Life Graphs, Epoch: 3/3, Loss: 2.9394\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 22/163 [01:47<13:07,  5.58s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Place Value, Epoch: 1/3, Loss: 3.5776\n","Parent: Place Value, Epoch: 2/3, Loss: 3.5339\n","Parent: Place Value, Epoch: 3/3, Loss: 3.4946\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|█▍        | 23/163 [01:55<14:49,  6.35s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Simplifying Expressions by Collecting Like Terms, Epoch: 1/3, Loss: 2.3425\n","Parent: Simplifying Expressions by Collecting Like Terms, Epoch: 2/3, Loss: 2.3425\n","Parent: Simplifying Expressions by Collecting Like Terms, Epoch: 3/3, Loss: 2.3425\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▍        | 24/163 [02:00<13:37,  5.88s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Simplifying Fractions, Epoch: 1/3, Loss: 1.7385\n","Parent: Simplifying Fractions, Epoch: 2/3, Loss: 1.7385\n","Parent: Simplifying Fractions, Epoch: 3/3, Loss: 1.7385\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▌        | 25/163 [02:04<12:10,  5.29s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Parallel Lines, Epoch: 1/3, Loss: 2.4184\n","Parent: Parallel Lines, Epoch: 2/3, Loss: 2.4184\n","Parent: Parallel Lines, Epoch: 3/3, Loss: 2.4184\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 26/163 [02:08<11:37,  5.09s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Dividing Fractions, Epoch: 1/3, Loss: 2.5895\n","Parent: Dividing Fractions, Epoch: 2/3, Loss: 2.5895\n","Parent: Dividing Fractions, Epoch: 3/3, Loss: 2.5895\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 27/163 [02:14<11:36,  5.12s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Properties of Triangles, Epoch: 1/3, Loss: 1.8091\n","Parent: Properties of Triangles, Epoch: 2/3, Loss: 1.8091\n","Parent: Properties of Triangles, Epoch: 3/3, Loss: 1.8091\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 28/163 [02:18<10:46,  4.79s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Function Machines, Epoch: 1/3, Loss: 2.6816\n","Parent: Function Machines, Epoch: 2/3, Loss: 2.5753\n","Parent: Function Machines, Epoch: 3/3, Loss: 2.5338\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 29/163 [02:24<11:46,  5.27s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Writing Expressions, Epoch: 1/3, Loss: 3.4146\n","Parent: Writing Expressions, Epoch: 2/3, Loss: 3.2857\n","Parent: Writing Expressions, Epoch: 3/3, Loss: 3.1784\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 30/163 [02:35<15:43,  7.09s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Fractions of an Amount, Epoch: 1/3, Loss: 2.2710\n","Parent: Fractions of an Amount, Epoch: 2/3, Loss: 2.2710\n","Parent: Fractions of an Amount, Epoch: 3/3, Loss: 2.2710\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▉        | 31/163 [02:40<14:08,  6.43s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Right-angled Triangles (SOHCAHTOA), Epoch: 1/3, Loss: 2.2937\n","Parent: Right-angled Triangles (SOHCAHTOA), Epoch: 2/3, Loss: 2.2937\n","Parent: Right-angled Triangles (SOHCAHTOA), Epoch: 3/3, Loss: 2.2937\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|█▉        | 32/163 [02:45<12:47,  5.86s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Inequalities on Number Lines, Epoch: 1/3, Loss: 0.6793\n","Parent: Inequalities on Number Lines, Epoch: 2/3, Loss: 0.6793\n","Parent: Inequalities on Number Lines, Epoch: 3/3, Loss: 0.6793\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 33/163 [02:49<11:25,  5.27s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Percentages of an Amount, Epoch: 1/3, Loss: 3.0869\n","Parent: Percentages of an Amount, Epoch: 2/3, Loss: 3.0634\n","Parent: Percentages of an Amount, Epoch: 3/3, Loss: 2.9930\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 34/163 [02:57<13:00,  6.05s/it]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/google-bertbert-base-uncased/transformers/default/1/cache/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Parent: Converting Mixed Number and Improper Fractions, Epoch: 1/3, Loss: 3.0766\n","Parent: Converting Mixed Number and Improper Fractions, Epoch: 2/3, Loss: 3.0766\n","Parent: Converting Mixed Number and Improper Fractions, Epoch: 3/3, Loss: 3.0766\n"]}],"source":["# Train Stage 2: Specific Misconception Classification\n","\n","train_stage2(model, train_df, model.tokenizer, hierarchy, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# After training, save \n","\n","model.save_model('model_checkpoint')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def evaluate_stage1(model, train_df, tokenizer, device):\n","    # Load parent model first\n","    model.load_parent_model(device)\n","    \n","    dataset = DistractorDataset(\n","        texts=train_df['AnswerText'].tolist(),\n","        labels=train_df['ParentCategoryEncoded'].tolist(),\n","        tokenizer=tokenizer,\n","        max_length=64  # Match training sequence length\n","    )\n","    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)  # Smaller batch size\n","    \n","    model.parent_model.eval()\n","    \n","    all_preds = []\n","    all_labels = []\n","    \n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            \n","            outputs = model.parent_model(input_ids=input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            preds = torch.argmax(logits, dim=1).cpu().numpy()\n","            \n","            all_preds.extend(preds)\n","            all_labels.extend(labels.cpu().numpy())\n","            \n","            # Free memory\n","            del outputs, logits, input_ids, attention_mask, labels\n","            torch.cuda.empty_cache()\n","    \n","    # Get unique parent categories and filter out None values\n","    target_names = [str(x) for x in train_df['ParentCategory'].unique() if pd.notna(x)]\n","    \n","    # Get unique label indices\n","    unique_labels = sorted(set(all_labels))\n","    \n","    print(classification_report(\n","        all_labels, \n","        all_preds, \n","        labels=unique_labels,\n","        target_names=target_names,\n","        zero_division=0\n","    ))\n","\n","def evaluate_stage2(model, train_df, tokenizer, hierarchy, device):\n","    for parent in hierarchy.parent_to_children.keys():\n","        parent_key = str(parent).replace(\" \", \"_\").replace(\"/\", \"_\")\n","        child_subset = train_df[train_df['ParentCategory'] == parent]\n","        \n","        if child_subset.empty:\n","            continue\n","        \n","        num_labels = len(child_subset['MisconceptionEncoded'].unique())\n","        \n","        try:\n","            # Load child model for this parent\n","            child_model = model.load_child_model(parent, num_labels, device)\n","            \n","            dataset = DistractorDataset(\n","                texts=child_subset['AnswerText'].tolist(),\n","                labels=child_subset['MisconceptionEncoded'].tolist(),\n","                tokenizer=tokenizer,\n","                max_length=64\n","            )\n","            dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n","            \n","            child_model.eval()\n","            \n","            all_preds = []\n","            all_labels = []\n","            all_probs = []\n","            \n","            with torch.no_grad():\n","                for batch in dataloader:\n","                    input_ids = batch['input_ids'].to(device)\n","                    attention_mask = batch['attention_mask'].to(device)\n","                    labels = batch['labels'].to(device)\n","                    \n","                    outputs = child_model(input_ids=input_ids, attention_mask=attention_mask)\n","                    logits = outputs.logits\n","                    probs = torch.softmax(logits, dim=1)\n","                    preds = torch.argmax(logits, dim=1).cpu().numpy()\n","                    \n","                    all_preds.extend(preds)\n","                    all_labels.extend(labels.cpu().numpy())\n","                    all_probs.extend(probs.cpu().numpy())\n","                    \n","                    # Free memory\n","                    del outputs, logits, probs, input_ids, attention_mask, labels\n","                    torch.cuda.empty_cache()\n","\n","            # Calculate MAP@25\n","            map25 = calculate_map25(all_labels, np.array(all_probs))\n","            print(f\"\\nMAP@25 Score for {parent}: {map25:.4f}\")\n","            \n","            # Get unique misconception names and filter out None values\n","            target_names = [str(x) for x in child_subset['MisconceptionName'].unique() if pd.notna(x)]\n","            \n","            # Get unique label indices\n","            unique_labels = sorted(set(all_labels))\n","            \n","            print(f\"\\nClassification Report for Parent Category: {parent}\")\n","            print(classification_report(\n","                all_labels, \n","                all_preds, \n","                labels=unique_labels,\n","                target_names=target_names,\n","                zero_division=0\n","            ))\n","            \n","        except Exception as e:\n","            print(f\"Error evaluating model for {parent}: {e}\")\n","            continue\n","        \n","        finally:\n","            # Clean up memory\n","            torch.cuda.empty_cache()\n","            gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Before prediction, load the model\n","model.load_model('model_checkpoint')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["evaluate_stage1(model, train_df, model.tokenizer, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["evaluate_stage2(model, train_df, model.tokenizer, hierarchy, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if log: print(\"(7) Melt test set\")\n","    \n","# Create a dictionary to map MisconceptionName to MisconceptionId\n","name_to_id = misconceptions.reset_index().set_index('MisconceptionName')['MisconceptionId'].to_dict()\n","\n","# Reshape the test DataFrame to have one row per QuestionId_Answer (A, B, C, D)\n","test_melted = test.melt(\n","    id_vars=['QuestionId', 'QuestionText', 'CorrectAnswer'],\n","    value_vars=['AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText'],\n","    var_name='AnswerOption',\n","    value_name='AnswerText'\n",")\n","\n","# Clean the 'AnswerOption' column to obtain A, B, C, D\n","test_melted['AnswerOption'] = test_melted['AnswerOption'].str.replace('Answer', '').str.replace('Text', '')\n","test_melted['QA_Id'] = test_melted['QuestionId'].astype(str) + '_' + test_melted['AnswerOption']\n","\n","# Drop correct answers\n","test_melted = test_melted[test_melted['CorrectAnswer'] != test_melted['AnswerOption']]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create submission predictions\n","def create_submission(model, test_df, tokenizer, hierarchy, label_encoders, device):\n","    # Ensure model is loaded\n","    if not hasattr(model, 'parent_model') or model.parent_model is None:\n","        model.load_model('model_checkpoint')\n","    \n","    # Get predictions for test data\n","    predictions = predict(model, test_melted['AnswerText'].tolist(), hierarchy, tokenizer, label_encoders, device)\n","    \n","    # Create submission DataFrame\n","    submission = pd.DataFrame({\n","        'Id': test_df.index,\n","        'MisconceptionName': predictions\n","    })\n","    \n","    # Save submission file\n","    submission.to_csv('submission.csv', index=False)\n","    if log:\n","        print(\"Submission file created successfully\")\n","    return submission"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["create_submission(model, test_melted, model.tokenizer, hierarchy, preprocessor.label_encoders, device)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":9738540,"sourceId":82695,"sourceType":"competition"},{"isSourceIdPinned":true,"modelId":177888,"modelInstanceId":155418,"sourceId":182326,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30805,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
