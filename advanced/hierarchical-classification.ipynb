{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = True\n",
    "log = True\n",
    "log_detail = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "import importlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allison/Downloads/idsn544/math-misconceptions/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) Imported data\n"
     ]
    }
   ],
   "source": [
    "if local:\n",
    "    misconceptions = pd.read_csv('../kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv', index_col='MisconceptionId')\n",
    "    train = pd.read_csv('../kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv')\n",
    "    test = pd.read_csv('../kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv')\n",
    "else:\n",
    "    misconceptions = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/misconception_mapping.csv', index_col='MisconceptionId')\n",
    "    train = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv')\n",
    "    test = pd.read_csv('/kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv')\n",
    "if log: print(\"(1) Imported data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Create train_melted \n",
    "#########################\n",
    "\n",
    "# Define the identifier columns\n",
    "id_cols = [\n",
    "    'QuestionId', 'ConstructId', 'ConstructName', \n",
    "    'SubjectId', 'SubjectName', 'CorrectAnswer', 'QuestionText'\n",
    "]\n",
    "\n",
    "# Define the corresponding Answer options\n",
    "answer_cols = ['AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText']\n",
    "misconception_cols = ['MisconceptionAId', 'MisconceptionBId', 'MisconceptionCId', 'MisconceptionDId']\n",
    "\n",
    "# Melt Answer Text\n",
    "text_melted = train.melt(\n",
    "    id_vars=id_cols,\n",
    "    value_vars=answer_cols,\n",
    "    var_name='Attribute',\n",
    "    value_name='AnswerText'\n",
    ")\n",
    "\n",
    "# Melt Misconception IDs\n",
    "misconception_melted = train.melt(\n",
    "    id_vars=id_cols,\n",
    "    value_vars=misconception_cols,\n",
    "    var_name='Attribute',\n",
    "    value_name='MisconceptionId'\n",
    ")\n",
    "\n",
    "# Extract the option letter (A, B, C, D) and the attribute type\n",
    "text_melted['AnswerOption'] = text_melted['Attribute'].str.extract(r'Answer([ABCD])Text')[0]\n",
    "misconception_melted['AnswerOption'] = misconception_melted['Attribute'].str.extract(r'Misconception([ABCD])Id')[0]\n",
    "\n",
    "# Drop the original 'Attribute' columns as they are no longer needed\n",
    "text_melted.drop('Attribute', axis=1, inplace=True)\n",
    "misconception_melted.drop('Attribute', axis=1, inplace=True)\n",
    "\n",
    "# Merge the two melted DataFrames on id_vars and AnswerOption\n",
    "train_melted = pd.merge(\n",
    "    text_melted,\n",
    "    misconception_melted,\n",
    "    on=id_cols + ['AnswerOption'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "train_melted = train_melted.merge(misconceptions, left_on='MisconceptionId', right_index=True, how='left')\n",
    "train_melted = train_melted[train_melted['CorrectAnswer'] != train_melted['AnswerOption']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubjectName nunique: 163\n",
      "ConstructName nunique: 757\n",
      "MisconceptionName nunique: 1604\n"
     ]
    }
   ],
   "source": [
    "print(f\"SubjectName nunique: {train_melted['SubjectName'].nunique()}\")\n",
    "print(f\"ConstructName nunique: {train_melted['ConstructName'].nunique()}\")\n",
    "print(f\"MisconceptionName nunique: {train_melted['MisconceptionName'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy_scm = {}\n",
    "for subject in train_melted['SubjectName'].unique():\n",
    "    subject_data = train_melted[train_melted['SubjectName'] == subject]\n",
    "    constructs = subject_data['ConstructName'].dropna().unique()\n",
    "    constructs_list = []\n",
    "    for construct in constructs:\n",
    "        misconceptions = subject_data[subject_data['ConstructName'] == construct]['MisconceptionName'].dropna().unique().tolist()\n",
    "        constructs_list.extend(misconceptions)\n",
    "    hierarchy_scm[subject] = constructs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hierarchy:\n",
    "    def __init__(self):\n",
    "        self.parent_to_children = {}\n",
    "        for subject, misconceptions in hierarchy_scm.items():\n",
    "            self.parent_to_children[subject] = misconceptions\n",
    "        self.child_to_parent = {child: parent for parent, children in self.parent_to_children.items() for child in children}\n",
    "    \n",
    "    def get_parent(self, child):\n",
    "        return self.child_to_parent.get(child, None)\n",
    "    \n",
    "    def get_children(self, parent):\n",
    "        return self.parent_to_children.get(parent, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, train_df, test_df, hierarchy):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.hierarchy = hierarchy\n",
    "        self.label_encoders = {}\n",
    "        self.child_label_encoders = {}\n",
    "    \n",
    "    def map_hierarchy(self):\n",
    "        self.train_df['ParentCategory'] = self.train_df['MisconceptionName'].apply(self.hierarchy.get_parent)\n",
    "        self.test_df['ParentCategory'] = None  # To be predicted later\n",
    "    \n",
    "    def encode_labels(self):\n",
    "        # Encode parent categories\n",
    "        le_parent = LabelEncoder()\n",
    "        self.train_df['ParentCategoryEncoded'] = le_parent.fit_transform(self.train_df['ParentCategory'])\n",
    "        self.label_encoders['parent'] = le_parent\n",
    "        \n",
    "        # Encode specific misconceptions per parent\n",
    "        self.child_label_encoders = {}\n",
    "        self.train_df['MisconceptionEncoded'] = -1  # Initialize with a placeholder\n",
    "        \n",
    "        for parent in self.hierarchy.parent_to_children.keys():\n",
    "            misconception_subset = self.train_df[self.train_df['ParentCategory'] == parent]['MisconceptionName']\n",
    "            le_child = LabelEncoder()\n",
    "            encoded = le_child.fit_transform(misconception_subset)\n",
    "            self.train_df.loc[self.train_df['ParentCategory'] == parent, 'MisconceptionEncoded'] = encoded\n",
    "            self.child_label_encoders[parent] = le_child\n",
    "        \n",
    "    def preprocess(self):\n",
    "        self.map_hierarchy()\n",
    "        self.encode_labels()\n",
    "        # Additional preprocessing steps like text cleaning can be added here\n",
    "        return self.train_df, self.test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalClassifier:\n",
    "    def __init__(self, parent_num_labels, child_num_labels, pretrained_model='bert-base-uncased'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "        self.parent_model = BertForSequenceClassification.from_pretrained(pretrained_model, num_labels=parent_num_labels)\n",
    "        self.child_models = nn.ModuleDict()\n",
    "        self.pretrained_model = pretrained_model\n",
    "    \n",
    "    def add_child_model(self, parent_category, num_labels):\n",
    "        # Convert parent category to valid dictionary key\n",
    "        parent_key = str(parent_category).replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        if parent_key not in self.child_models:\n",
    "            try:\n",
    "                self.child_models[parent_key] = BertForSequenceClassification.from_pretrained(\n",
    "                    self.pretrained_model, \n",
    "                    num_labels=num_labels\n",
    "                )\n",
    "                if log:\n",
    "                    print(f\"Created child model for {parent_category} with {num_labels} labels\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating child model for {parent_category}: {str(e)}\")\n",
    "                raise\n",
    "                \n",
    "    def forward_parent(self, input_ids, attention_mask):\n",
    "        return self.parent_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    def forward_child(self, parent_category, input_ids, attention_mask):\n",
    "        child_model = self.child_models.get(parent_category, None)\n",
    "        if child_model:\n",
    "            return child_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistractorDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_stage1(model, train_df, tokenizer, device, epochs=3):\n",
    "    dataset = DistractorDataset(\n",
    "        texts=train_df['AnswerText'].tolist(),\n",
    "        labels=train_df['ParentCategoryEncoded'].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parent_model.parameters(), lr=2e-5)\n",
    "    model.parent_model.to(device)\n",
    "    model.parent_model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "        \n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model.parent_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_examples += labels.size(0)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = total_correct / total_examples\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "def train_stage2(model, train_df, tokenizer, hierarchy, device, epochs=3):\n",
    "    for parent in tqdm(hierarchy.parent_to_children.keys()):\n",
    "        # Convert parent name to valid dictionary key right away\n",
    "        parent_key = str(parent).replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        \n",
    "        child_subset = train_df[train_df['ParentCategory'] == parent]\n",
    "        if child_subset.empty:\n",
    "            if log: print(f\"Skipping {parent} - no data\")\n",
    "            continue\n",
    "            \n",
    "        # Get number of unique misconceptions for this parent\n",
    "        num_labels = len(child_subset['MisconceptionEncoded'].unique())\n",
    "        \n",
    "        # Initialize the child model if it doesn't exist\n",
    "        try:\n",
    "            model.add_child_model(parent, num_labels)\n",
    "            if log: print(f\"Successfully added child model for {parent}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to add child model for {parent}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            dataset = DistractorDataset(\n",
    "                texts=child_subset['AnswerText'].tolist(),\n",
    "                labels=child_subset['MisconceptionEncoded'].tolist(),\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "            dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "            \n",
    "            # Now we can safely access the child model\n",
    "            child_model = model.child_models[parent_key].to(device)\n",
    "            optimizer = torch.optim.AdamW(child_model.parameters(), lr=2e-5)\n",
    "            child_model.train()\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                total_loss = 0\n",
    "                for batch in dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "                    \n",
    "                    # Ensure labels are Long type\n",
    "                    labels = labels.long()\n",
    "                    \n",
    "                    outputs = child_model(\n",
    "                        input_ids=input_ids, \n",
    "                        attention_mask=attention_mask, \n",
    "                        labels=labels\n",
    "                    )\n",
    "                    \n",
    "                    loss = outputs.loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                avg_loss = total_loss / len(dataloader)\n",
    "                if log:\n",
    "                    print(f\"Parent: {parent}, Epoch: {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training model for {parent}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, texts, hierarchy, tokenizer, label_encoders, device):\n",
    "    dataset = DistractorDataset(\n",
    "        texts=texts,\n",
    "        labels=[0]*len(texts),  # Dummy labels\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    model.parent_model.to(device)\n",
    "    model.parent_model.eval()\n",
    "    \n",
    "    parent_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model.parent_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            parent_preds.extend(preds)\n",
    "    \n",
    "    parent_labels = label_encoders['parent'].inverse_transform(parent_preds)\n",
    "    specific_preds = []\n",
    "    \n",
    "    for i, parent in enumerate(parent_labels):\n",
    "        child_model = model.child_models.get(parent, None)\n",
    "        if child_model is None:\n",
    "            specific_preds.append(None)\n",
    "            continue\n",
    "        child_model.to(device)\n",
    "        child_model.eval()\n",
    "        with torch.no_grad():\n",
    "            encoding = tokenizer.encode_plus(\n",
    "                texts[i],\n",
    "                add_special_tokens=True,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            outputs = child_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "            # Use the specific encoder for the parent\n",
    "            specific_misconception = label_encoders['child_label_encoders'][parent].inverse_transform([pred])[0]\n",
    "            specific_preds.append(specific_misconception)\n",
    "    \n",
    "    return specific_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy = Hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to your data files\n",
    "train_path = '../kaggle/input/eedi-mining-misconceptions-in-mathematics/train.csv'\n",
    "test_path = '../kaggle/input/eedi-mining-misconceptions-in-mathematics/test.csv'\n",
    "\n",
    "# Initialize DataPreprocessor\n",
    "preprocessor = DataPreprocessor(train_df=train_melted, test_df=test, hierarchy=hierarchy)\n",
    "train_df, test_df = preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allison/Downloads/idsn544/math-misconceptions/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of parent labels\n",
    "parent_num_labels = train_df['ParentCategoryEncoded'].nunique()\n",
    "\n",
    "# Initialize HierarchicalClassifier\n",
    "model = HierarchicalClassifier(parent_num_labels=parent_num_labels, child_num_labels=0)  # child_num_labels handled per parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 176/176 [11:57<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Loss: 4.4837, Accuracy: 0.2056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 176/176 [11:21<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Loss: 4.1692, Accuracy: 0.2204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 176/176 [12:10<00:00,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Loss: 3.9742, Accuracy: 0.2256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train Stage 1: Parent Category Classification\n",
    "train_stage1(model, train_df, model.tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/163 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Simplifying Algebraic Fractions with 8 labels\n",
      "Successfully added child model for Simplifying Algebraic Fractions\n",
      "Parent: Simplifying Algebraic Fractions, Epoch: 1/3, Average Loss: 2.1719\n",
      "Parent: Simplifying Algebraic Fractions, Epoch: 2/3, Average Loss: 2.1498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/163 [00:07<19:43,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Simplifying Algebraic Fractions, Epoch: 3/3, Average Loss: 2.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Range and Interquartile Range from a List of Data with 9 labels\n",
      "Successfully added child model for Range and Interquartile Range from a List of Data\n",
      "Parent: Range and Interquartile Range from a List of Data, Epoch: 1/3, Average Loss: 2.1838\n",
      "Parent: Range and Interquartile Range from a List of Data, Epoch: 2/3, Average Loss: 2.0885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/163 [00:17<23:27,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Range and Interquartile Range from a List of Data, Epoch: 3/3, Average Loss: 1.9776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Properties of Quadrilaterals with 12 labels\n",
      "Successfully added child model for Properties of Quadrilaterals\n",
      "Parent: Properties of Quadrilaterals, Epoch: 1/3, Average Loss: 2.5574\n",
      "Parent: Properties of Quadrilaterals, Epoch: 2/3, Average Loss: 2.4912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/163 [00:27<25:35,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Properties of Quadrilaterals, Epoch: 3/3, Average Loss: 2.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Area of Simple Shapes with 30 labels\n",
      "Successfully added child model for Area of Simple Shapes\n",
      "Parent: Area of Simple Shapes, Epoch: 1/3, Average Loss: 3.4360\n",
      "Parent: Area of Simple Shapes, Epoch: 2/3, Average Loss: 3.3548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/163 [00:58<47:53, 18.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Area of Simple Shapes, Epoch: 3/3, Average Loss: 3.3694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Converting between Fractions and Percentages with 9 labels\n",
      "Successfully added child model for Converting between Fractions and Percentages\n",
      "Parent: Converting between Fractions and Percentages, Epoch: 1/3, Average Loss: 2.2229\n",
      "Parent: Converting between Fractions and Percentages, Epoch: 2/3, Average Loss: 2.2380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/163 [01:06<37:28, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Converting between Fractions and Percentages, Epoch: 3/3, Average Loss: 2.1076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Transformations of functions in the form f(x) with 8 labels\n",
      "Successfully added child model for Transformations of functions in the form f(x)\n",
      "Parent: Transformations of functions in the form f(x), Epoch: 1/3, Average Loss: 2.1281\n",
      "Parent: Transformations of functions in the form f(x), Epoch: 2/3, Average Loss: 2.1011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 6/163 [01:14<32:12, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Transformations of functions in the form f(x), Epoch: 3/3, Average Loss: 2.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Expanding Triple Brackets and more with 1 labels\n",
      "Successfully added child model for Expanding Triple Brackets and more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 7/163 [01:15<22:22,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error training model for Expanding Triple Brackets and more: Found dtype Long but expected Float\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Nets with 3 labels\n",
      "Successfully added child model for Nets\n",
      "Parent: Nets, Epoch: 1/3, Average Loss: 0.9563\n",
      "Parent: Nets, Epoch: 2/3, Average Loss: 1.0263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 8/163 [01:26<23:58,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Nets, Epoch: 3/3, Average Loss: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Time with 22 labels\n",
      "Successfully added child model for Time\n",
      "Parent: Time, Epoch: 1/3, Average Loss: 3.1132\n",
      "Parent: Time, Epoch: 2/3, Average Loss: 2.9969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 9/163 [01:54<38:42, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Time, Epoch: 3/3, Average Loss: 2.8255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Trial and Improvement and Iterative Methods with 4 labels\n",
      "Successfully added child model for Trial and Improvement and Iterative Methods\n",
      "Parent: Trial and Improvement and Iterative Methods, Epoch: 1/3, Average Loss: 1.3917\n",
      "Parent: Trial and Improvement and Iterative Methods, Epoch: 2/3, Average Loss: 1.3095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 10/163 [02:01<32:15, 12.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Trial and Improvement and Iterative Methods, Epoch: 3/3, Average Loss: 1.2855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Sharing in a Ratio with 14 labels\n",
      "Successfully added child model for Sharing in a Ratio\n",
      "Parent: Sharing in a Ratio, Epoch: 1/3, Average Loss: 2.6100\n",
      "Parent: Sharing in a Ratio, Epoch: 2/3, Average Loss: 2.6116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 11/163 [02:18<35:05, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Sharing in a Ratio, Epoch: 3/3, Average Loss: 2.5673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Ordering Negative Numbers with 6 labels\n",
      "Successfully added child model for Ordering Negative Numbers\n",
      "Parent: Ordering Negative Numbers, Epoch: 1/3, Average Loss: 1.8938\n",
      "Parent: Ordering Negative Numbers, Epoch: 2/3, Average Loss: 1.8721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 12/163 [02:22<27:25, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Ordering Negative Numbers, Epoch: 3/3, Average Loss: 1.7641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Adding and Subtracting Negative Numbers with 14 labels\n",
      "Successfully added child model for Adding and Subtracting Negative Numbers\n",
      "Parent: Adding and Subtracting Negative Numbers, Epoch: 1/3, Average Loss: 2.7335\n",
      "Parent: Adding and Subtracting Negative Numbers, Epoch: 2/3, Average Loss: 2.6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 13/163 [02:38<31:22, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Adding and Subtracting Negative Numbers, Epoch: 3/3, Average Loss: 2.6122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Adding and Subtracting Algebraic Fractions with 6 labels\n",
      "Successfully added child model for Adding and Subtracting Algebraic Fractions\n",
      "Parent: Adding and Subtracting Algebraic Fractions, Epoch: 1/3, Average Loss: 1.8197\n",
      "Parent: Adding and Subtracting Algebraic Fractions, Epoch: 2/3, Average Loss: 1.8453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 14/163 [02:43<25:39, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Adding and Subtracting Algebraic Fractions, Epoch: 3/3, Average Loss: 1.7952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Volume of Prisms with 15 labels\n",
      "Successfully added child model for Volume of Prisms\n",
      "Parent: Volume of Prisms, Epoch: 1/3, Average Loss: 2.7741\n",
      "Parent: Volume of Prisms, Epoch: 2/3, Average Loss: 2.6150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 15/163 [03:04<33:13, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Volume of Prisms, Epoch: 3/3, Average Loss: 2.3449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Factorising into a Double Bracket with 11 labels\n",
      "Successfully added child model for Factorising into a Double Bracket\n",
      "Parent: Factorising into a Double Bracket, Epoch: 1/3, Average Loss: 2.4166\n",
      "Parent: Factorising into a Double Bracket, Epoch: 2/3, Average Loss: 2.2167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 16/163 [03:24<37:35, 15.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Factorising into a Double Bracket, Epoch: 3/3, Average Loss: 2.1690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Mental Multiplication and Division with 15 labels\n",
      "Successfully added child model for Mental Multiplication and Division\n",
      "Parent: Mental Multiplication and Division, Epoch: 1/3, Average Loss: 2.7263\n",
      "Parent: Mental Multiplication and Division, Epoch: 2/3, Average Loss: 2.7472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 17/163 [03:38<36:16, 14.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Mental Multiplication and Division, Epoch: 3/3, Average Loss: 2.6816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Substitution into Formula with 11 labels\n",
      "Successfully added child model for Substitution into Formula\n",
      "Parent: Substitution into Formula, Epoch: 1/3, Average Loss: 2.4476\n",
      "Parent: Substitution into Formula, Epoch: 2/3, Average Loss: 2.4342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 18/163 [03:49<33:48, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Substitution into Formula, Epoch: 3/3, Average Loss: 2.3224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Counting with 2 labels\n",
      "Successfully added child model for Counting\n",
      "Parent: Counting, Epoch: 1/3, Average Loss: 0.7792\n",
      "Parent: Counting, Epoch: 2/3, Average Loss: 0.6412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 19/163 [03:56<28:01, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Counting, Epoch: 3/3, Average Loss: 0.5694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Solving Linear Inequalities with 7 labels\n",
      "Successfully added child model for Solving Linear Inequalities\n",
      "Parent: Solving Linear Inequalities, Epoch: 1/3, Average Loss: 1.8493\n",
      "Parent: Solving Linear Inequalities, Epoch: 2/3, Average Loss: 1.6458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 20/163 [04:04<25:41, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Solving Linear Inequalities, Epoch: 3/3, Average Loss: 1.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Linear Equations with 24 labels\n",
      "Successfully added child model for Linear Equations\n",
      "Parent: Linear Equations, Epoch: 1/3, Average Loss: 3.2502\n",
      "Parent: Linear Equations, Epoch: 2/3, Average Loss: 3.1858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 21/163 [04:24<31:43, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Linear Equations, Epoch: 3/3, Average Loss: 3.1399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Real Life Graphs with 21 labels\n",
      "Successfully added child model for Real Life Graphs\n",
      "Parent: Real Life Graphs, Epoch: 1/3, Average Loss: 3.0997\n",
      "Parent: Real Life Graphs, Epoch: 2/3, Average Loss: 2.8934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 22/163 [04:41<33:48, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Real Life Graphs, Epoch: 3/3, Average Loss: 2.8252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Place Value with 34 labels\n",
      "Successfully added child model for Place Value\n",
      "Parent: Place Value, Epoch: 1/3, Average Loss: 3.6548\n",
      "Parent: Place Value, Epoch: 2/3, Average Loss: 3.4810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 23/163 [05:05<40:28, 17.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Place Value, Epoch: 3/3, Average Loss: 3.4017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Simplifying Expressions by Collecting Like Terms with 9 labels\n",
      "Successfully added child model for Simplifying Expressions by Collecting Like Terms\n",
      "Parent: Simplifying Expressions by Collecting Like Terms, Epoch: 1/3, Average Loss: 2.1564\n",
      "Parent: Simplifying Expressions by Collecting Like Terms, Epoch: 2/3, Average Loss: 2.1104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 24/163 [05:16<36:01, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Simplifying Expressions by Collecting Like Terms, Epoch: 3/3, Average Loss: 2.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Simplifying Fractions with 5 labels\n",
      "Successfully added child model for Simplifying Fractions\n",
      "Parent: Simplifying Fractions, Epoch: 1/3, Average Loss: 1.7915\n",
      "Parent: Simplifying Fractions, Epoch: 2/3, Average Loss: 1.6366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 25/163 [05:22<28:58, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Simplifying Fractions, Epoch: 3/3, Average Loss: 1.5208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Parallel Lines with 10 labels\n",
      "Successfully added child model for Parallel Lines\n",
      "Parent: Parallel Lines, Epoch: 1/3, Average Loss: 2.4631\n",
      "Parent: Parallel Lines, Epoch: 2/3, Average Loss: 2.3015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 26/163 [05:32<27:21, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Parallel Lines, Epoch: 3/3, Average Loss: 2.2118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Dividing Fractions with 13 labels\n",
      "Successfully added child model for Dividing Fractions\n",
      "Parent: Dividing Fractions, Epoch: 1/3, Average Loss: 2.6681\n",
      "Parent: Dividing Fractions, Epoch: 2/3, Average Loss: 2.6102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 27/163 [05:47<28:45, 12.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Dividing Fractions, Epoch: 3/3, Average Loss: 2.5682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Properties of Triangles with 6 labels\n",
      "Successfully added child model for Properties of Triangles\n",
      "Parent: Properties of Triangles, Epoch: 1/3, Average Loss: 1.7846\n",
      "Parent: Properties of Triangles, Epoch: 2/3, Average Loss: 1.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 28/163 [05:53<24:21, 10.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Properties of Triangles, Epoch: 3/3, Average Loss: 1.6033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Function Machines with 14 labels\n",
      "Successfully added child model for Function Machines\n",
      "Parent: Function Machines, Epoch: 1/3, Average Loss: 2.6767\n",
      "Parent: Function Machines, Epoch: 2/3, Average Loss: 2.6060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 29/163 [06:11<28:56, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Function Machines, Epoch: 3/3, Average Loss: 2.5689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Writing Expressions with 29 labels\n",
      "Successfully added child model for Writing Expressions\n",
      "Parent: Writing Expressions, Epoch: 1/3, Average Loss: 3.4039\n",
      "Parent: Writing Expressions, Epoch: 2/3, Average Loss: 3.1983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 30/163 [06:51<46:26, 20.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Writing Expressions, Epoch: 3/3, Average Loss: 3.0541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Fractions of an Amount with 10 labels\n",
      "Successfully added child model for Fractions of an Amount\n",
      "Parent: Fractions of an Amount, Epoch: 1/3, Average Loss: 2.3144\n",
      "Parent: Fractions of an Amount, Epoch: 2/3, Average Loss: 2.2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 31/163 [07:00<38:34, 17.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Fractions of an Amount, Epoch: 3/3, Average Loss: 2.1895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Right-angled Triangles (SOHCAHTOA) with 8 labels\n",
      "Successfully added child model for Right-angled Triangles (SOHCAHTOA)\n",
      "Parent: Right-angled Triangles (SOHCAHTOA), Epoch: 1/3, Average Loss: 2.0320\n",
      "Parent: Right-angled Triangles (SOHCAHTOA), Epoch: 2/3, Average Loss: 1.9780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 32/163 [07:10<33:06, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Right-angled Triangles (SOHCAHTOA), Epoch: 3/3, Average Loss: 1.7732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Inequalities on Number Lines with 2 labels\n",
      "Successfully added child model for Inequalities on Number Lines\n",
      "Parent: Inequalities on Number Lines, Epoch: 1/3, Average Loss: 0.7722\n",
      "Parent: Inequalities on Number Lines, Epoch: 2/3, Average Loss: 0.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 33/163 [07:16<27:07, 12.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Inequalities on Number Lines, Epoch: 3/3, Average Loss: 0.6574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Percentages of an Amount with 22 labels\n",
      "Successfully added child model for Percentages of an Amount\n",
      "Parent: Percentages of an Amount, Epoch: 1/3, Average Loss: 3.1214\n",
      "Parent: Percentages of an Amount, Epoch: 2/3, Average Loss: 3.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 34/163 [07:36<31:42, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Percentages of an Amount, Epoch: 3/3, Average Loss: 2.9237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Converting Mixed Number and Improper Fractions with 18 labels\n",
      "Successfully added child model for Converting Mixed Number and Improper Fractions\n",
      "Parent: Converting Mixed Number and Improper Fractions, Epoch: 1/3, Average Loss: 2.9157\n",
      "Parent: Converting Mixed Number and Improper Fractions, Epoch: 2/3, Average Loss: 2.7771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 35/163 [07:49<30:05, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Converting Mixed Number and Improper Fractions, Epoch: 3/3, Average Loss: 2.8432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Ordering Fractions with 10 labels\n",
      "Successfully added child model for Ordering Fractions\n",
      "Parent: Ordering Fractions, Epoch: 1/3, Average Loss: 2.2195\n",
      "Parent: Ordering Fractions, Epoch: 2/3, Average Loss: 2.1442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 36/163 [08:00<28:10, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Ordering Fractions, Epoch: 3/3, Average Loss: 2.0376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Averages and Range from Frequency Table with 16 labels\n",
      "Successfully added child model for Averages and Range from Frequency Table\n",
      "Parent: Averages and Range from Frequency Table, Epoch: 1/3, Average Loss: 2.9153\n",
      "Parent: Averages and Range from Frequency Table, Epoch: 2/3, Average Loss: 2.7518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 37/163 [08:14<27:59, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Averages and Range from Frequency Table, Epoch: 3/3, Average Loss: 2.6982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Quadratic Equations with 21 labels\n",
      "Successfully added child model for Quadratic Equations\n",
      "Parent: Quadratic Equations, Epoch: 1/3, Average Loss: 3.1028\n",
      "Parent: Quadratic Equations, Epoch: 2/3, Average Loss: 2.9156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 38/163 [08:43<37:40, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Quadratic Equations, Epoch: 3/3, Average Loss: 2.8919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Adding and Subtracting with Decimals with 10 labels\n",
      "Successfully added child model for Adding and Subtracting with Decimals\n",
      "Parent: Adding and Subtracting with Decimals, Epoch: 1/3, Average Loss: 2.3375\n",
      "Parent: Adding and Subtracting with Decimals, Epoch: 2/3, Average Loss: 2.2289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 39/163 [08:57<34:51, 16.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent: Adding and Subtracting with Decimals, Epoch: 3/3, Average Loss: 2.2230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Experimental Probability and Relative Frequency with 1 labels\n",
      "Successfully added child model for Experimental Probability and Relative Frequency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 40/163 [08:58<24:48, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error training model for Experimental Probability and Relative Frequency: Found dtype Long but expected Float\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created child model for Horizontal and Vertical Lines with 8 labels\n",
      "Successfully added child model for Horizontal and Vertical Lines\n"
     ]
    }
   ],
   "source": [
    "# Train Stage 2: Specific Misconception Classification\n",
    "train_stage2(model, train_df, model.tokenizer, hierarchy, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directory to save models\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save parent model\n",
    "model.parent_model.save_pretrained('models/parent_model')\n",
    "\n",
    "# Save child models\n",
    "for parent, child_model in model.child_models.items():\n",
    "    model.child_models[parent].save_pretrained(f'models/child_model_{parent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_stage1(model, train_df, tokenizer, device):\n",
    "    dataset = DistractorDataset(\n",
    "        texts=train_df['DistractorText'].tolist(),\n",
    "        labels=train_df['ParentCategoryEncoded'].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    model.parent_model.to(device)\n",
    "    model.parent_model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model.parent_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    print(classification_report(all_labels, all_preds, target_names=preprocessor.label_encoders['parent'].classes_))\n",
    "\n",
    "# Evaluate Stage 1\n",
    "evaluate_stage1(model, train_df, model.tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_stage2(model, train_df, tokenizer, hierarchy, device):\n",
    "    for parent in hierarchy.parent_to_children.keys():\n",
    "        child_subset = train_df[train_df['ParentCategory'] == parent]\n",
    "        if child_subset.empty:\n",
    "            continue\n",
    "        \n",
    "        child_model = model.child_models.get(parent, None)\n",
    "        if child_model is None:\n",
    "            continue\n",
    "        \n",
    "        child_model.to(device)\n",
    "        child_model.eval()\n",
    "        \n",
    "        dataset = DistractorDataset(\n",
    "            texts=child_subset['DistractorText'].tolist(),\n",
    "            labels=child_subset['MisconceptionEncoded'].tolist(),\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = child_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        print(f\"Classification Report for Parent Category: {parent}\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=preprocessor.label_encoders['child'].classes_))\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Evaluate Stage 2\n",
    "evaluate_stage2(model, train_df, model.tokenizer, hierarchy, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
